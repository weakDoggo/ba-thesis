% !TEX root = ../main.tex
%
\chapter{Conclusion}
\label{sec:conclusion}

% Structure
% Review 
% Outlook 

% To be placed 


\section{Review}
\label{sec:conclusion:review}

Our study examined the impact of various regularization techniques, particularly \ac{gdc}, on graph-level prediction tasks across two different network types.
Specifically, we postulated the following three research questions, which we will now answer.

% Here come explicit answers to all the research questions
\paragraph{RQ1: Is regularization, specifically \acs*{gdc}, effective in solving the problem of over-fitting and over-smoothing for graph-level prediction tasks?}
Since both our networks had no problem with over-fitting, even if no regularization was used, we cannot say anything about the effectiveness of regularization in mitigating this problem.
We can only say that the model is not negatively affected by regularization in terms of over-smoothing.

\paragraph{RQ2: Is there a difference in performance between \acs*{gcn} and \acs*{gin} architectures regarding performance with regularization techniques?}

We found no difference in performance between both network architectures \ac{gcn} and \ac{gin}.
Both networks perform best when we use no regularization at all, and on both networks, \ac{de} is the second-best performance most of the time.
This holds for both classification and regression datasets.


\paragraph{RQ3: Are there similarities and differences between different regularization techniques in terms of performance?}
We tried to verify or refute whether any of the four regularization techniques \ac{do}, \ac{ns}, \ac{de}, and \ac{gdc} is more effective.
In terms of performance, the different regularization techniques are very close.
\Ac{de} performs best among the different datasets and \ac{gdc} is second-place, hinting at the close relatedness between those two techniques, as already has been described by \citep{Hasanzadeh2020}.


\section{Future Work}
\label{sec:conclusion:future}

In our study, we found out that, indeed, for graph-level prediction tasks, regularization is not very helpful; we think that several interesting questions arise from there.

First, we looked at two well-known architectures which are commonly used.
However, we performed our experiments only on a limited number of datasets, leaving the possibility that results could differ on other datasets.

We introduced an interesting metric, which unfortunately fell out of scope in this research.
We encourage others to pick up where we left...



And there is the possibility, however small, that there are other types of regularization, that could perform well
Another proposed regularization technique
https://arxiv.org/pdf/2106.07971.pdf