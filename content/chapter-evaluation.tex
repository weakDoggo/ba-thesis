% !TEX root = ../main.tex
%
\chapter{Evaluation}
\label{sec:eval}

% Is it necessary to quote all the studies here ?? YES
In this section, we delve into the critical evaluation of the machine learning experiments conducted as part of this research.
The main postulated question of this study was to determine whether \ac{gdc} is effective in solving the problem of overfitting and over-smoothing for graph-level prediction tasks, as there is already a wide range of conducted studies, which answer the question of various regularization techniques for node level prediction tasks. Other regularization techniques have also been evaluated for this type of task.
The investigation encompassed classification and regression tasks, with a comprehensive analysis of two types of neural networks, \ac{gcn} and \ac{gin}.
The datasets of choice were all molecular datasets.

In the evaluation, we will mainly focus on two manipulated parameters: the number of layers and the dropout rate, since the number of layers is important in concluding overfitting, especially the issue where additional layers do not make the network perform better. The dropout rate since this parameter indicates the efficacy of various types of dropouts and if higher rates have an impact at all.





\begin{table*}[t]
    \caption{
        Experimental results for graph-level prediction tasks. With ROC-AUC metric for OGB-molhiv, AP for -molpcba and MSE for the three remaining regression datasets.
    }\label{tbl:eval:results}
    \centering
    {\small%\scriptsize%
        \csvreader[
            column count=12,
            tabular={clrrrrr},
            separator=semicolon,
            table head={%
                    & \multicolumn{1}{l}{} &%
                    \multicolumn{1}{c}{\textbf{OGB-molhiv}} &%
                    \multicolumn{1}{c}{\textbf{-molpcba}} &%
                    \multicolumn{1}{c}{\textbf{-molesol}} &%
                    \multicolumn{1}{c}{\textbf{-molfreesolv}} &%
                    \multicolumn{1}{c}{\textbf{-mollipo}}%
                    \\\toprule%
                },
            before reading={\setlength{\tabcolsep}{4pt}},
            table foot=\bottomrule,
            late after line=\ifthenelse{\equal{\id}{5}}{\\\midrule}{\\},
            head to column names
        ]{data/results.csv}{}{%
            \ifthenelse{\equal{\id}{0}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\textsc{GCN}}}}{}%
            \ifthenelse{\equal{\id}{5}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\textsc{GIN}}}}{}&%
            \textbf{\regularization} &%
            {\evalres{0}{\molhivAvg}{\molhivStd}} &%
            {\evalres{0}{\molpcbaAvg}{\molpcbaStd}} &%
            {\evalres{0}{\molesolAvg}{\molesolStd}} &%
            {\evalres{0}{\molfreesolvAvg}{\molfreesolvStd}} &%
            {\evalres{0}{\mollipoAvg}{\mollipoStd}}%
        }}
\end{table*}

% Description of results - Basic overview 
Some insights we can gather from the results table.
% Basic first view insights: 
The best results are achieved using no regularization techniques for graph-level prediction tasks on both types of networks \ac{gcn} and \ac{gin}. This holds for both datasets - classification and regression- indicating that any regularization type is unsuitable for graph-level prediction tasks independently of the network of choice.

For both classification datasets, the variance is very small; the network performance is stable. Out of the three regression datasets, only the mollipo dataset has low variance in performance for both types of \acp{gnn}, and we have rather a high variance on the remaining datasets. \ac{gcn} and \ac{gin} perform better on classification tasks for graph-level predictions.


% Among the datasets 
We cannot point out a clear 'winner' among the different regularization methods on both networks and across all datasets.
As all the results are very close in range, we cannot point out any notable advantages of using \ac{gdc} above other regularization methods. Their performance varies depending on the dataset, with different methods performing better on certain datasets.

% GCN patterns 

There is an interesting pattern, that we find in the performance of the \ac{gdc} network:
On the classification datasets, the second best performance is achieved by \ac{de} followed by \ac{gdc} and






% Overall conclusion 

Our study examined the impact of various regularization techniques, particularly \ac{gdc}, on graph-level prediction tasks across two different network types. Regularization techniques are commonly employed in node-prediction tasks across diverse networks to mitigate over-smoothing and overfitting issues. These methods perturb the values and introduce randomness, leading to improved results.

In graph-level tasks, the final output is a readout of aggregated nodes, making over-smoothing less problematic as the focus shifts from distinguishing individual nodes to capturing collective behavior. Consequently, introducing controlled randomness might seem unnecessary for such predictive tasks. This shift in focus from individual nodes to aggregated outcomes challenges the conventional wisdom of applying regularization methods in graph-level prediction tasks.

Despite this counterintuitive aspect, our study aimed to empirically validate the effectiveness of regularization techniques in graph-level prediction tasks. Our exploration of this uncharted territory was motivated by a desire to challenge existing assumptions and gain a nuanced understanding of the relationship between regularization methods and graph-level predictions. This unconventional approach enabled a rigorous assessment of current methodologies, providing valuable insights into graph-based machine learning.


\section{Datasets and Metrics Overview}

Before proceeding to the experimental findings, we present a quick overview of the used datasets and metrics to ensure a better understanding of the subject matter. We have used five datasets, all from the molecular realm. Molhiv and molpcba are small and medium-size classification datasets, respectively.

The other three, OGB-molesol, -mollipo and -molfreesolv are regression datasets. They contain 1128, 4200, and 642
molecular structure graphs, respectively. The regression task is to predict the solubility
of a molecule in different substances.
To evaluate the performance of molhiv, we used ROC-AUC, for molpcba AP was used and we used MAE for all the regression datasets.

% \subsection{metrics and meaning}
% ROC-AUC: The better value for ROC-AUC is a value closer to 1, with values over 0,5 being better than random guessing.
% AP (when class distribution on a dataset are imbalanced): Bigger values of AP are better
% MAE: The smaller the better.

\newcommand*{\addstd}[4]{
    \addplot[name path=#3upper, draw=none] table[x=#2, y expr=\thisrow{#3Avg}+\thisrow{#3Std}, col sep=semicolon] {#1};
    \addplot[name path=#3lower, draw=none] table[x=#2, y expr=\thisrow{#3Avg}-\thisrow{#3Std}, col sep=semicolon] {#1};
    \addplot[fill=#4, fill opacity=0.1] fill between[of=#3upper and #3lower];
}
\newcommand*{\layerplot}[4]{\begin{tikzpicture}
        \begin{axis}[
                width=0.8\linewidth,
                height=6cm,
                xlabel={#3},
                ylabel={#4},
                legend pos=outer north east,
                legend style={nodes={scale=0.6, transform shape}},
                grid=major,
            ]

            % Plot the data from the CSV file
            \addplot[color= p_red] table [x=#2, y=noneAvg, col sep=semicolon] {#1};
            \addlegendentry{No Regularization}

            \addplot[color= p_green] table [x=#2, y=dropoutAvg, col sep=semicolon] {#1};
            \addlegendentry{Dropout}

            \addplot[color= p_blue] table [x=#2, y=nodesamplingAvg, col sep=semicolon] {#1};
            \addlegendentry{Node Sampling}

            \addplot[color= p_yellow] table [x=#2, y=dropedgeAvg, col sep=semicolon] {#1};
            \addlegendentry{DropEdge}

            \addplot[color= p_violet] table [x=#2, y=gdcAvg, col sep=semicolon] {#1};
            \addlegendentry{GDC}

            \addstd{#1}{#2}{none}{p_red}
            \addstd{#1}{#2}{dropout}{p_green}
            \addstd{#1}{#2}{nodesampling}{p_blue}
            \addstd{#1}{#2}{dropedge}{p_yellow}
            \addstd{#1}{#2}{gdc}{p_violet}
        \end{axis}
    \end{tikzpicture}}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molfreesolv_gcn.csv}{numLayers}{Number of Layers}{MSE}
    \caption{molfreesolv (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}
About the GCN molfreesolv plot. The molfreesolv plot is the

% GIN 
% GIN MOLHIV 

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molhiv_gin.csv}{numLayers}{Number of Layers}{MSE}
    \caption{molhiv (GIN Model)}
\end{figure}


\end{document}
