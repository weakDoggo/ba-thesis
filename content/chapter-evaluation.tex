% !TEX root = ../main.tex
%
\chapter{Evaluation}
\label{sec:eval}

In this section, we delve into the critical evaluation of the machine learning experiments conducted as part of this research.
The main postulated question of this study was to determine whether \ac{gdc} is effective in solving the problem of overfitting and over-smoothing not only for node-level prediction tasks but also for graph-level prediction tasks. Other regularization techniques have also been evaluated for this type of task.
The investigation encompassed classification and regression tasks, with a comprehensive analysis of two types of neural networks, \ac{gcn} and \ac{gin}.
The datasets of choice were all molecular datasets.

In the evaluation, we will mainly focus on two manipulated parameters: the number of layers and the dropout rate, since the number of layers is important in concluding overfitting, especially the issue where additional layers do not make the network perform better. The dropout rate since this parameter indicates the efficacy of various types of dropouts and if higher rates have an impact at all.





\begin{table*}[t]
    \caption{
        Experimental results for graph-level prediction tasks. With ROC-AUC metric for OGB-molhiv, AP for -molpcba and MSE for the three remaining regression datasets.
    }\label{tbl:eval:results}
    \centering
    {\small%\scriptsize%
        \csvreader[
            column count=12,
            tabular={clrrrrr},
            separator=semicolon,
            table head={%
                    & \multicolumn{1}{l}{} &%
                    \multicolumn{1}{c}{\textbf{OGB-molhiv}} &%
                    \multicolumn{1}{c}{\textbf{-molpcba}} &%
                    \multicolumn{1}{c}{\textbf{-molesol}} &%
                    \multicolumn{1}{c}{\textbf{-molfreesolv}} &%
                    \multicolumn{1}{c}{\textbf{-mollipo}}%
                    \\\toprule%
                },
            before reading={\setlength{\tabcolsep}{4pt}},
            table foot=\bottomrule,
            late after line=\ifthenelse{\equal{\id}{5}}{\\\midrule}{\\},
            head to column names
        ]{data/results.csv}{}{%
            \ifthenelse{\equal{\id}{0}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\textsc{GCN}}}}{}%
            \ifthenelse{\equal{\id}{5}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\textsc{GIN}}}}{}&%
            \textbf{\regularization} &%
            {\evalres{0}{\molhivAvg}{\molhivStd}} &%
            {\evalres{0}{\molpcbaAvg}{\molpcbaStd}} &%
            {\evalres{0}{\molesolAvg}{\molesolStd}} &%
            {\evalres{0}{\molfreesolvAvg}{\molfreesolvStd}} &%
            {\evalres{0}{\mollipoAvg}{\mollipoStd}}%
        }}
\end{table*}

Some insights we can gather from the results table.
% Basic first view insights: 
% GCN 
Starting with the \ac{gdc} network, we see, that the best results are always achieved when using no regularization techniques at all. This holds true for all classification as well as all the regression datasets, with the overall variance of the model performance being rather small, except on the molesol and the mmolfreesolv datasets. As all the result s are very close in range, we cannot point out any notable advantages of using \ac{gdc} above other regularization methods as they performance varies depending on the dataset with different methods performing better on certain datasets.

% GIN 





\subsection{datasets and metrics overview}

Before proceeding to the experimental findings, we present a quick overview of the used datasets and metrics to ensure a better understanding of the subject matter. We have used five datasets, all from the molecular realm. Molhiv and molpcba are small and medium-size classification datasets, respectively.

The other three, OGB-molesol, -mollipo and -molfreesolv are regression datasets. They contain 1128, 4200, and 642
molecular structure graphs, respectively. The regression task is to predict the solubility
of a molecule in different substances.
To evaluate the performance of molhiv, we used ROC-AUC, for molpcba AP was used and we used MAE for all the regression datasets.

% \subsection{metrics and meaning}
% ROC-AUC: The better value for ROC-AUC is a value closer to 1, with values over 0,5 being better than random guessing.
% AP (when class distribution on a dataset are imbalanced): Bigger values of AP are better
% MAE: The smaller the better.

\newcommand*{\addstd}[4]{
    \addplot[name path=#3upper, draw=none] table[x=#2, y expr=\thisrow{#3Avg}+\thisrow{#3Std}, col sep=semicolon] {#1};
    \addplot[name path=#3lower, draw=none] table[x=#2, y expr=\thisrow{#3Avg}-\thisrow{#3Std}, col sep=semicolon] {#1};
    \addplot[fill=#4, fill opacity=0.1] fill between[of=#3upper and #3lower];
}
\newcommand*{\layerplot}[4]{\begin{tikzpicture}
        \begin{axis}[
                width=0.8\linewidth,
                height=6cm,
                xlabel={#3},
                ylabel={#4},
                legend pos=outer north east,
                legend style={nodes={scale=0.6, transform shape}},
                grid=major,
            ]

            % Plot the data from the CSV file
            \addplot[color= p_red] table [x=#2, y=noneAvg, col sep=semicolon] {#1};
            \addlegendentry{No Regularization}

            \addplot[color= p_green] table [x=#2, y=dropoutAvg, col sep=semicolon] {#1};
            \addlegendentry{Dropout}

            \addplot[color= p_blue] table [x=#2, y=nodesamplingAvg, col sep=semicolon] {#1};
            \addlegendentry{Node Sampling}

            \addplot[color= p_yellow] table [x=#2, y=dropedgeAvg, col sep=semicolon] {#1};
            \addlegendentry{DropEdge}

            \addplot[color= p_violet] table [x=#2, y=gdcAvg, col sep=semicolon] {#1};
            \addlegendentry{GDC}

            \addstd{#1}{#2}{none}{p_red}
            \addstd{#1}{#2}{dropout}{p_green}
            \addstd{#1}{#2}{nodesampling}{p_blue}
            \addstd{#1}{#2}{dropedge}{p_yellow}
            \addstd{#1}{#2}{gdc}{p_violet}
        \end{axis}
    \end{tikzpicture}}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molfreesolv_gcn.csv}{numLayers}{Number of Layers}{MSE}
    \caption{molfreesolv (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}
About the GCN molfreesolv plot. The molfreesolv plot is the

% GIN 
% GIN MOLHIV 

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molhiv_gin.csv}{numLayers}{Number of Layers}{MSE}
    \caption{molhiv (GIN Model)}
\end{figure}


\end{document}
