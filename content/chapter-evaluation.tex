% !TEX root = ../main.tex
%
\chapter{Evaluation}
\label{sec:eval}

% Is it necessary to quote all the studies here ?? YES
In this section, we delve into the critical evaluation of the machine learning experiments conducted as part of this research.
The main postulated question of this study was to determine whether \ac{gdc} is effective in solving the problem of overfitting and over-smoothing for graph-level prediction tasks, as there is already a wide range of conducted studies, which answer the question of various regularization techniques for node level prediction tasks. Other regularization techniques have also been evaluated for this type of task.
The investigation encompassed classification and regression tasks, with a comprehensive analysis of two types of neural networks, \ac{gcn} and \ac{gin}.
The datasets of choice were all molecular datasets.

In the evaluation, we will mainly focus on two manipulated parameters: the number of layers and the dropout rate, since the number of layers is important in concluding overfitting, especially the issue where additional layers do not make the network perform better. The dropout rate since this parameter indicates the efficacy of various types of dropouts and if higher rates have an impact at all.

\begin{table*}[t]
    \caption{
        Experimental results for graph-level prediction tasks. With ROC-AUC metric for OGB-molhiv, AP for -molpcba and MSE for the three remaining regression datasets.
    }\label{tbl:eval:results}
    \centering
    {\small%\scriptsize%
        \csvreader[
            column count=12,
            tabular={clrrrrr},
            separator=semicolon,
            table head={%
                    & \multicolumn{1}{l}{} &%
                    \multicolumn{1}{c}{\textbf{OGB-molhiv}} &%
                    \multicolumn{1}{c}{\textbf{-molpcba}} &%
                    \multicolumn{1}{c}{\textbf{-molesol}} &%
                    \multicolumn{1}{c}{\textbf{-molfreesolv}} &%
                    \multicolumn{1}{c}{\textbf{-mollipo}}%
                    \\\toprule%
                },
            before reading={\setlength{\tabcolsep}{4pt}},
            table foot=\bottomrule,
            late after line=\ifthenelse{\equal{\id}{5}}{\\\midrule}{\\},
            head to column names
        ]{data/results.csv}{}{%
            \ifthenelse{\equal{\id}{0}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\textsc{GCN}}}}{}%
            \ifthenelse{\equal{\id}{5}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\textsc{GIN}}}}{}&%
            \textbf{\regularization} &%
            {\evalres{0}{\molhivAvg}{\molhivStd}} &%
            {\evalres{0}{\molpcbaAvg}{\molpcbaStd}} &%
            {\evalres{0}{\molesolAvg}{\molesolStd}} &%
            {\evalres{0}{\molfreesolvAvg}{\molfreesolvStd}} &%
            {\evalres{0}{\mollipoAvg}{\mollipoStd}}%
        }}
\end{table*}

\section{Datasets and Metrics Overview}

Before proceeding to the experimental findings, we present a quick overview of the used datasets and metrics to ensure a better understanding of the subject matter. We have used five datasets, all from the molecular realm. Molhiv and molpcba are small and medium-size classification datasets, respectively.

The other three, OGB-molesol, -mollipo and -molfreesolv are regression datasets. They contain 1128, 4200, and 642
molecular structure graphs, respectively. The regression task is to predict the solubility
of a molecule in different substances.
To evaluate the performance of molhiv, we used ROC-AUC, for molpcba AP was used and we used MAE for all the regression datasets.

% Description of results - Basic overview 
Some insights we can gather from the results table:
% Basic first view insights: 
The best results are achieved using no regularization techniques for graph-level prediction tasks on both types of networks \ac{gcn} and \ac{gin}. This holds for both datasets -- classification and regression -- indicating that any regularization type is unsuitable for both graph-level prediction tasks independently of the network of choice.

For both classification datasets, the variance is very small; the network performance is stable. Out of the three regression datasets, only the mollipo dataset has low variance in performance for both types of \acp{gnn}, and we have rather a high variance on the remaining datasets. \ac{gcn} and \ac{gin} perform better on classification tasks for graph-level predictions. This high variance of molesol and mollipo is an interesting trend, which would be nice to investigate further.

% Patterns Among the datasets 
Despite achieving the best result when using no regularization, we can point out a clear second-place `winner' among the different regularization methods on both networks and across all datasets.\ac{de} performs the second best in all cases, with the second place being \ac{gdc} for classification tasks and \ac{ns} for regression tasks, apart from one exception on the mollipo dataset where the second best performing regularization is \ac{gdc}. However, as all the results are very close in range, we cannot point out any notable advantages of using \ac{gdc} above other regularization methods, as their performance varies depending on the task and dataset.
% Other noteworthy things  
Despite the \ac{gin} network being more powerful than \ac{gcn} and as powerful as the 1-dim\ac{wl} test, the performance on both networks is very similar, with only a significant difference in performance on the molhiv dataset.

% Overall conclusion 
\section{Conclusion}

Our study examined the impact of various regularization techniques, particularly \ac{gdc}, on graph-level prediction tasks across two different network types. Regularization techniques are commonly employed in node-prediction tasks across diverse networks to mitigate over-smoothing and overfitting issues. These methods perturb the values and introduce randomness, leading to improved results.

In graph-level tasks, the final output is a readout of aggregated nodes, making over-smoothing less problematic as the focus shifts from distinguishing individual nodes to capturing collective behavior. Consequently, introducing controlled randomness might seem unnecessary for such predictive tasks. This shift in focus from individual nodes to aggregated outcomes challenges the conventional wisdom of applying regularization methods in graph-level prediction tasks.

Despite this counterintuitive aspect, our study aimed to empirically validate the effectiveness of regularization techniques in graph-level prediction tasks. Our exploration of this uncharted territory was motivated by a desire to challenge existing assumptions and gain a nuanced understanding of the relationship between regularization methods and graph-level predictions. This unconventional approach enabled a rigorous assessment of current methodologies, providing valuable insights into graph-based machine learning.

\newcommand*{\addstd}[4]{
    \addplot[name path=#3upper, draw=none] table[x=#2, y expr=\thisrow{#3Avg}+\thisrow{#3Std}, col sep=semicolon] {#1};
    \addplot[name path=#3lower, draw=none] table[x=#2, y expr=\thisrow{#3Avg}-\thisrow{#3Std}, col sep=semicolon] {#1};
    \addplot[fill=#4, fill opacity=0.1] fill between[of=#3upper and #3lower];
}
\newcommand*{\layerplot}[6]{\begin{tikzpicture}
        \begin{axis}[
                width=0.8\linewidth,
                height=6cm,
                xlabel={#3},
                ylabel={#4},
                xtick distance={#5}, % Set x-axis tick distance
                ytick distance={#6}, % Set y-axis tick distance
                legend pos=outer north east,
                legend style={nodes={scale=0.6, transform shape}},
                grid=major,
            ]

            % Plot the data from the CSV file
            \addplot[color= p_red] table [x=#2, y=noneAvg, col sep=semicolon] {#1};
            \addlegendentry{No Regularization}

            \addplot[color= p_green] table [x=#2, y=dropoutAvg, col sep=semicolon] {#1};
            \addlegendentry{Dropout}

            \addplot[color= p_blue] table [x=#2, y=nodesamplingAvg, col sep=semicolon] {#1};
            \addlegendentry{Node Sampling}

            \addplot[color= p_yellow] table [x=#2, y=dropedgeAvg, col sep=semicolon] {#1};
            \addlegendentry{DropEdge}

            \addplot[color= p_violet] table [x=#2, y=gdcAvg, col sep=semicolon] {#1};
            \addlegendentry{GDC}

            \addstd{#1}{#2}{none}{p_red}
            \addstd{#1}{#2}{dropout}{p_green}
            \addstd{#1}{#2}{nodesampling}{p_blue}
            \addstd{#1}{#2}{dropedge}{p_yellow}
            \addstd{#1}{#2}{gdc}{p_violet}
        \end{axis}
    \end{tikzpicture}}




\section{Insights from Plots}
This analysis aims to gain insights into the phenomenon of over-smoothing. To achieve this, we investigate the variations in performance corresponding to changes in the number of layers within the network architecture.

\subsection{Insights from Classification Datasets}
\subsubsection{Effect of the Number of Layers}
Upon examining five distinct datasets, notable trends emerge concerning the performance relative to the number of layers in the network. In four out of five datasets, a consistent pattern is observed.
% Number of Layers GCN

Increasing the number of layers demonstrates a positive correlation with performance in the cases of molpcba, molesol, and molfreesolv datasets. The peak performance is typically attained at layer 5 or 6. It is important to highlight that while there are discernible differences, they tend to be relatively small across layers 3 to 6. Since this trend holds for all regularization techniques, including no regularization at all indiscriminately, there is no evidence in favor of regularization solving the problem of over-smoothing for graph-level classification tasks on the \ac{gcn}, at least for those particular datasets.

% Number of Layers GIN 
On the \ac{gin} network, we cannot see any occurring pattern or trend as to whether various regularization techniques solve the problem of over-smoothing. With a few exceptions, using regularization leads to smaller performance variance. Regularization also seems to smooth the curve, meaning the overall difference from layer to layer is smoother - with a [here comes the trend]. This is slight, but
good indicator favoring the beneficial connection between over-smoothing and regularization techniques solving this issue.


% Patterns of variance in datasets 

Among the datasets, the classification dataset molhiv and the regression dataset molpcba

% GCN Network number of layers PLOTS
% NEW PLOTS try 2 


\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molhiv_gcn.csv}{numLayers}{Number of Layers}{ROC-AUC}{1}{0.1}
    \caption{molhiv (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molpcba_gcn.csv}{numLayers}{Number of Layers}{AP}{1}{0.02}
    \caption{molpcba (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}


\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molesol_gcn.csv}{numLayers}{Number of Layers}{MSE}{1}{1}
    \caption{molesol(GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molfreesolv_gcn.csv}{numLayers}{Number of Layers}{MSE}{1}{5}
    \caption{molfreesolv (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_mollipo_gcn.csv}{numLayers}{Number of Layers}{MSE}{1}{0.2}
    \caption{mollipo (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\subsubsection{Probability Manipulation}


% Classification Datasets Insights Num Layers 
\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molhiv_gin.csv}{numLayers}{Nuber of Layers}{ROC-AUC}{1}{0.1}
    \caption{molhiv (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molpcba_gin.csv}{numLayers}{Nuber of Layers}{AP}{1}{0.95}
    \caption{molpcba (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

% Regression Datasets Insights 
\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molesol_gin.csv}{numLayers}{Nuber of Layers}{MSE}{1}{1}
    \caption{molesol (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_molfreesolv_gin.csv}{numLayers}{Nuber of Layers}{MSE}{1}{5}
    \caption{molfreesolv (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/num_layers_plot_mollipo_gin.csv}{numLayers}{Nuber of Layers}{MSE}{1}{0.2}
    \caption{mollipo (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}
\section{Insights Regression Datasets}


% PROBABILITY 
% HERE ARE THE DATASETS FOR PROBABILITY ON GCN


We can present some interesting findings concerning the probability, with which features are dropped. The probability does not have any effect on the performance. The values are constant for all probabilites 0.3, 0.5 and 0.7. for both types of networks. On the \ac{gcn} network the variance is extremely high on the molhiv as well as the molfreesolv dataset, on all the other datasets we have a rather stable performance. On the \ac{gin} network we have the following results:



\begin{figure}
    \centering
    \layerplot{data/probability_plot_molhiv_gcn.csv}{probability}{Probability}{ROC-AUC}{0.2}{0.1}
    \caption{molhiv (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_molpcba_gcn.csv}{probability}{Probability}{AP}{0.2}{0.02}
    \caption{molpcba (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_molesol_gcn.csv}{probability}{Probability}{MSE}{0.2}{1}
    \caption{molesol (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_molfreesolv_gcn.csv}{probability}{Probability}{MSE}{0.2}{2}
    \caption{molfreesolv (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_mollipo_gcn.csv}{probability}{Probability}{MSE}{0.2}{0.2}
    \caption{mollipo (GCN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

% HERE ARE THE DATASETS FOR PROBABiLITY ON GIN 
\begin{figure}
    \centering
    \layerplot{data/probability_plot_molhiv_gin.csv}{probability}{Probability}{ROC-AUC}{0.2}{0.1}
    \caption{molhiv (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_molpcba_gin.csv}{probability}{Probability}{AP}{0.2}{0.02}
    \caption{molpcba (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_molesol_gin.csv}{probability}{Probability}{MSE}{0.2}{1}
    \caption{molesol (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_molfreesolv_gin.csv}{probability}{Probability}{MSE}{0.2}{2}
    \caption{molfreesolv (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}

\begin{figure}
    \centering
    \layerplot{data/probability_plot_mollipo_gin.csv}{probability}{Probability}{MSE}{0.2}{0.2}
    \caption{mollipo (GIN Model)}
    \label{fig:gcn-molfreesolv}
\end{figure}



% Overall Conclusion for the number of layers/ regularization 
As mentioned earlier, the results are very close in range, and together with the limited number of datasets- all from the same molecular realm- we cannot draw any general conclusions. However, we can describe some emerging trends from our findings:
The trends:
Overall, regularization seems to smoothen the line, i.e., to reduce the differences in performance depending on the number of layers. Also, the performance variance is much higher when we use no regularization.

\end{document}
