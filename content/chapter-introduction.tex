% !TEX root = ../main.tex
% chktex-file 46
\chapter{Introduction}%
\label{sec:intro}

\pagenumbering{arabic}			% arabic page numbering
\setcounter{page}{1}			% set page counter

The field of \acs{ml} on graph-structured data has recently become an active topic of research.
One reason for this is the wide range of domains and problems that are expressible regarding graphs.
Regularization techniques are commonly employed in node-level prediction tasks across diverse networks to mitigate over-smoothing and overfitting issues.
These methods perturb the values and introduce randomness, leading to improved results.
In graph-level tasks, however, the final output is a readout of aggregated nodes, hypothetically minimizing the impact of over-smoothing as the emphasis shifts from distinguishing individual nodes to capturing collective behavior.


\section{Motivation}%
\label{sec:intro:motivation}
% Mention GDC (including citation!)

The exploration of controlled randomness in graph-level prediction tasks may initially appear unconventional since the focus shifts from individual nodes to capturing the collective behavior of the graph.
The usual thinking about graph-level predictions might question if we need controlled randomness.
Despite this counterintuitive aspect, our study aimed to empirically validate the effectiveness of regularization techniques in graph-level prediction tasks.
Our curiosity was motivated by a desire to challenge existing assumptions and gain a nuanced understanding of the relationship between regularization methods and graph-level predictions.
In this pursuit, we aim to extend the boundaries of understanding regarding the impact of regularization on aggregated outcomes.
One noteworthy technique that caught our attention was \acf{gdc}, which introduces stochasticity through adaptive connection sampling \cite{Hasanzadeh2020}.
By drawing different random masks for each channel and edge independently, \ac{gdc} promises to provide nuanced improvements, surpassing the effectiveness of traditional methods or even their combinations.
Also, we want to better understand the role of controlled randomness in the context of over-fitting and over-smoothing.
By evaluating four regularization techniques

\section{Research Questions}%
\label{sec:intro:questions}
In this thesis, we will answer the following research questions to assess the relevance of regularization for graph-level prediction tasks:
\begin{enumerate}
    \item Is regularization, specifically \ac{gdc}, effective in solving the problem of over-fitting and over-smoothing for graph-level prediction tasks?
    \item Is there a difference in performance between \ac{gcn} and \ac{gin} architectures regarding performance with regularization techniques?
    \item Are there similarities and differences between different regularization techniques in terms of performance?
\end{enumerate}
\section{Structure}%
\label{sec:intro:structure}

\paragraph{\Cref{sec:related}: \nameref{sec:related}}
In order to answer our three research questions, we first take a closer look at three common prediction tasks in \acp{gnn}
and give a general overview of how \acp{gnn} organize and process graph-structured data.
We discuss the relation of message passing mechanism to the \ac{wl}-test and give a formal definition and description of two \acp{gnn} architectures, \ac{gcn} and \ac{gin}, before taking a closer look at typical issues occurring in \acp{gnn}.
Finally, we present four regularization techniques, which mitigate two commonly occurring problems and \acf{mad}, a measure for smoothness between nodes\cite{Chen2020}.
% Problem Description + Implementation = implement 
\paragraph{\Cref{sec:implement}: \nameref{sec:implement}}
This section overviews the implementation of graph convolutions using TensorFlows \texttt{gather} and \texttt{scatter} operations.
We then explain how we implement four regularization techniques by masking values during different convolution steps.
We provide an intuitive approach for looking at \ac{gdc}.
Lastly, we discuss the benefits of using \ac{ogb} datasets.
% eval contains discussion 
\paragraph{\Cref{sec:eval}: \nameref{sec:eval}}
We start with an overview of used datasets and metrics before proceeding to the experimental setup.
We then describe our parameter grid and explain how we choose the best set of hyperparameters.
Finally, we present the experimental results and provide detailed insights into how different numbers of layers and probability affect the performances of \ac{gnn} and \ac{gin} and draw a conclusion from our findings.
\paragraph{\Cref{sec:conclusion}: \nameref{sec:conclusion}}
Finally, the results of this thesis are summarized, and a brief outline of promising directions for future research is given.
